\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lmodern}
\usepackage{pgf,tikz}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{dsfont}
\usepackage{gensymb}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\title{Homework 2}
\author{Robien \textsc{Clerc} -- Pierre \textsc{Vigier}}

\begin{document}

\maketitle

\section{Randomized analysis}

\textbf{1a} Each vertex is removed with probability $1 - \frac{1}{d}$, thus remains with probability $\frac{1}{d}$.

Let us define for each $v \in V$, $X_v = \mathds{1}_{v\text{ remains}}$.

Let $N_V = \sum_{v \in V}{X_v}$ be the number of remaining vertices. Then, we immediately have:

$$
\mathbb{E}(N_V) = \sum_{v \in V}{\mathbb{E}(X_v)} = n Pr(v \text{ remains}) = \frac{n}{d}
$$

Let us define for each $e \in E$, $Y_e = \mathds{1}_{e\text{ remains}}$. But an edge remains if and only if the two vertices it connects remain so for each $e = (u, v)$, $Y_e = X_uX_v$.

Let $N_E = \sum_{e \in E}{Y_e}$ be the number of remaining edges. 

We have by independence of the $(X_v)_{v \in V}$ that $\mathbb{E}(Y_e) = \mathbb{E}(X_u) \mathbb{E}(X_v) = \frac{1}{d^2}$.

Thus we have $\mathbb{E}(N_E) = \frac{n \times d}{2} \times \mathbb{E}(Y_e) =\frac{n \times d}{2} \times \frac{1}{d^2} = \frac{n}{2d}$.

\textbf{2a} Let us consider the graph composed of the remaining vertices and edges from the previous question. It is composed in expectation of $\frac{n}{d}$ vertices and $\frac{n}{2d}$ edges.

We can remove all the remaining edges by removing only one of its two vertices. In expectation there are $\mathbb{E}(N_V - N_E) = \frac{n}{d} - \frac{n}{2d} = \frac{n}{2d}$ vertices remaining and no edges between them : this is an independent set.

There is at least one of those subsets with a value at least the expectation. It proves that G contains an independent set of size $\frac{n}{2d}$.

\section{Locality Sensitive Hashing}

\textbf{2a} We suppose that the points are in $\mathbb{R}^2$ and that $\arg(p)$ is the angle in degrees of a point with respect to the $x$ axis.

Let us define sets of angles of length $\alpha$ and offset $\theta$: $S_{\theta} = \{ \theta + \phi \mod 360, \phi \in [0, \alpha[ \}$.

Let us define functions $h_{\theta}$ such that:

$$
h_{\theta}(p) = \left\{\begin{array}{ll}
1 & \text{ if } \arg(p) \in S_{\theta} \\
0 & \text{ otherwise}
\end{array}\right.
$$

Let $\mathcal{H}$ be the family generated when $\theta \sim \mathcal{U}([0, 360[)$.

\textbf{2b} First let us find a lower bound on $Pr(h(p) = h(q))$ when $d(p, q) \leq 1\degree$.

\begin{alignat*}{2}
Pr(h(p) \neq h(q)) & = Pr(\{h(p) = 1, h(q) = 0\} \cup \{h(p) = 0, h(q) = 1\}) \\
& = Pr(h(p) = 1, h(q) = 0) + Pr(h(p) = 0, h(q) = 1) \\
& = 2 \times Pr(h(p) = 1, h(q) = 0) \\
\end{alignat*}

We have that $Pr(h(p) = 1) = \frac{\alpha}{360}$. Moreover $Pr(h(q) = 0 | h(p) = 1) = \frac{d(p, q)}{\alpha} \leq \frac{1}{\alpha}$. So:

$$
Pr(h(p) = 1, h(q) = 0) = Pr(h(p) = 1)Pr(h(q) = 0 | h(p) = 1)\leq \frac{1}{360}
$$

Finally, we have that:

$$
Pr(h(p) = h(q)) = 1 - Pr(h(p) \neq h(q)) \geq 1 - \frac{2}{360} \geq 0.99
$$

\textbf{2c} Now let us find an upper bound on $Pr(h(p) = h(q))$ when $d(p, q) \geq 10\degree$.

If $\alpha \leq 10$, then we can not have $\arg(p) \in S_{\theta}$ and $\arg(q) \in S_{\theta}$ at the same time. So $Pr(h(q) = 0 | h(p) = 1) = 1$ hence:

$$
Pr(h(p) \neq h(q)) = 2Pr(h(p) = 1, h(q) = 0) = 2Pr(h(p) = 1) = \frac{2\alpha}{360}
$$

Thus:

$$
Pr(h(p) = h(q)) = 1 - \frac{2\alpha}{360}
$$

\textbf{2d} To conclude we have to find $\alpha \leq 10$ such that $1 - \frac{2\alpha}{360} \leq 0.95$.

If we take $\alpha = 10\degree$ then $1 - \frac{2\alpha}{360} \approx 0.944$.

\section{Estimating the number of collisions}

\textbf{3a} $Z = {m \choose 2}\mathds{1}_{h(A_i) = h(A_j)}$ where $\{A_i, A_j\}$ is picked uniformly at random among all pairs of songs. But there is ${m \choose 2}$ such pairs so $Pr(\{A_i, A_j\} = \{a_i, a_j\}) = \frac{1}{{m \choose 2}}$.

Thus:

\begin{alignat*}{2}
\mathbb{E}(Z) & = \sum_{\{a_i, a_j\}}{Pr(\{A_i, A_j\} = \{a_i, a_j\}){m \choose 2}\mathds{1}_{h(a_i) = h(a_j)}} \\
& = \frac{{m \choose 2}}{{m \choose 2}}\sum_{\{a_i, a_j\}}{\mathds{1}_{h(a_i) = h(a_j)}} \\
& = \text{\# collisions}
\end{alignat*}

$\mathds{1}_{h(A_i) = h(A_j)}$ is a Bernoulli variable so:

$$
\text{Var}(\mathds{1}_{h(A_i) = h(A_j)}) = \mathbb{E}(\mathds{1}_{h(A_i) = h(A_j)})(1-\mathbb{E}(\mathds{1}_{h(A_i) = h(A_j)})) \leq \mathbb{E}(\mathds{1}_{h(A_i) = h(A_j)})
$$

Thus:

\begin{alignat*}{2}
\text{Var}(Z) & = {m \choose 2}^2 \text{Var}(\mathds{1}_{h(A_i) = h(A_j)}) \\
& \leq {m \choose 2}^2 \mathbb{E}(\mathds{1}_{h(A_i) = h(A_j)}) \\
& = {m \choose 2} \mathbb{E}({m \choose 2}\mathds{1}_{h(A_i) = h(A_j)}) \\
& = {m \choose 2} \mathbb{E}(Z) \\
& = {m \choose 2} (\text{\# collisions})
\end{alignat*}

\textbf{3b} Let $(Z_{i,j})_{i \in \{1, ..., k\}, j \in \{1, ..., l\}}$ be $k \times l$ independent copies of $Z$.

Then let $Z_i = \frac{1}{l}\sum_{j=1}^l{Z_{i,j}}$ be the average of $l$ copies for all $i \in \{1, ..., k\}$.

Finally we use the median trick by defining $\hat{Z} = \text{median}(Z_1, ..., Z_k)$.

First let us study the $(Z_i)$. We have that $\mathbb{E}(Z_i) = \mathbb{E}(Z) = \text{\# collisions}$ and $\text{Var}(Z_i) = \frac{1}{l}\text{Var}(Z) \leq \frac{{m \choose 2} (\text{\# collisions})}{l}$.

Thus according to the Chebyshev inequality:

\begin{alignat*}{2}
Pr(|Z_i - \text{\# collisions}| & \geq \epsilon (\text{\# collisions})) \leq \frac{\text{Var}(Z_i)}{(\epsilon (\text{\# collisions}))^2} \\
& \leq \frac{\frac{{m \choose 2} (\text{\# collisions})}{l}}{(\epsilon (\text{\# collisions}))^2} \\
& =  \frac{{m \choose 2}}{l \epsilon^2 (\text{\# collisions})}
\end{alignat*}

The number of collisions is minimized when each bucket has approximately the same number of elements. So the minimum number of collisions is something like $n{\frac{m}{n} \choose 2}$. Let us prove the following lemma to formalize this intuition.

\begin{lemma}
$\text{\# collisions} \geq nf(\frac{m}{n})$ with $f(x) = \frac{x(x-1)}{2}$.
\end{lemma}

\begin{proof}
Let $N_i$ be the number of elements in the buckets $i$, we have:

$$
\text{\# collisions} = \sum_{i = 1}^n{{N_i \choose 2}} = \sum_{i = 1}^n{f(N_i)} = n\sum_{i = 1}^n{\frac{1}{n}f(N_i)}
$$

$f''(x) = 1 \geq 0$ so f is convex and we have:

$$
\text{\# collisions} \geq n f(\sum_{i = 1}^n{\frac{1}{n}N_i}) = n f(\frac{1}{n}\sum_{i = 1}^n{N_i}) = n f(\frac{m}{n})
$$

because $\sum_{i = 1}^n{N_i} = m$
\end{proof}

In addition as $m \geq 2n$, we have:

$$
nf(\frac{m}{n}) = n\frac{\frac{m}{n}(\frac{m}{n}-1)}{2} = \frac{m(m-n)}{2n} \geq \frac{m^2}{4n} \geq \frac{1}{2n}{m \choose 2}
$$

So $\text{\# collisions} \geq \frac{1}{2n}{m \choose 2}$ and if we inject this inequality in the one obtained from the Chebyshev inequality we have:

$$
Pr(|Z_i - \text{\# collisions}| \geq \epsilon (\text{\# collisions})) \leq \frac{{m \choose 2}}{l \epsilon^2 (\frac{1}{2n}{m \choose 2})} = \frac{2n}{l \epsilon^2}
$$

Hence if we take $l \geq \frac{6n}{\epsilon^2}$ then $Pr(|Z_i - \text{\# collisions}| \geq \epsilon (\text{\# collisions})) \leq \frac{1}{3}$.

Now, let us analyze $\hat{Z}$. Let $X_i = \mathds{1}_{|Z_i - \text{\# collisions}| \geq \epsilon (\text{\# collisions})}$. The $X_i$'s are independent Bernoulli variables and we have shown that $\mathbb{E}(X_i) \leq \frac{1}{3}$.

Let $X = \sum_{i = 1}^k{X_i}$. We have $\mu = \mathbb{E}(X) \leq \frac{k}{3}$. So according to one of the Chernoff bounds:

$$
Pr(X \geq \frac{k}{2}) = Pr(X \geq \mu(1 + \frac{1}{2})) \leq \exp(-\frac{(\frac{1}{2})^2}{3}\mu) \leq \exp(-\frac{k}{36})
$$

But:

$$
Pr(|\hat{Z} - \text{\# collisions}| \geq \epsilon (\text{\# collisions})) \leq Pr(X \geq \frac{k}{2}) \leq \exp(-\frac{k}{36})
$$

Hence we have $Pr(|\hat{Z} - \text{\# collisions}| \leq \epsilon (\text{\# collisions})) \leq 1 - \delta$ if $k \geq 36\ln(\frac{1}{\delta})$.

Thus we only need to download $k \times l = 216\frac{n}{\epsilon^2}\ln(\frac{1}{\delta}) = O(\frac{n}{\epsilon^2}\ln(\frac{1}{\delta}))$ songs to achieve the required performance.

\section{Tutte matrix}

Our team name is "French Fries". The number of our accepted submission is
38454771.

\section{Karger's min-cut algorithm}

Our team name is "French Fries". The number of our accepted submission is
38439376.

\end{document}